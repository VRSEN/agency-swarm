---
title: "Deployment to Production"
description: "Step-by-step guide for deploying your agency in a production environment."
icon: "rocket-launch"
sidebarTitle: "Deploy"
---

**Recommended:** Use the [Starter Template](/welcome/getting-started/starter-template) for production. It ships with FastAPI endpoints, auth, and a clean project layout.

## Required Environment Variables

Before deploying, ensure these are set in your production environment:

| Variable | Required | Description |
|----------|----------|-------------|
| `OPENAI_API_KEY` | Yes | Your OpenAI API key |
| `APP_TOKEN` | Recommended | Authentication token for FastAPI endpoints |

<Note>
Thread persistence uses callbacks you define to store threads in any database you choose.
</Note>

<Note>
  This guide assumes you have already created an agency. If you haven't, check out the [Getting Started](/welcome/installation) guide.
</Note>

<Warning>
  Before deploying, ensure you have thoroughly tested all tools and agents. Run the test cases in each tool file and verify the agency works end-to-end using demo methods.
</Warning>

## Deployment Process

<Steps>

<Step title="Step 1: Persist Conversation Threads" icon="message-dots">

By default, every time you create a new `Agency()`, it starts a fresh conversation thread. In production, you usually need to resume prior conversations or handle multiple users.

<Info>
ThreadManager stores a flat list of messages with agent metadata, including agent-to-agent messages.
</Info>

Chat persistence is handled through callback functions passed to the Agency constructor:

```python
def save_threads(thread_dict: list[dict[str, TResponseInputItem]], chat_id: str):
    # Save updated threads to your database
    save_threads_to_db(thread_dict)

def load_threads(chat_id: str) -> list[dict[str, TResponseInputItem]]:
    return load_threads_from_db(chat_id)

agency = Agency(
    agent1,
    agent2,
    communication_flows=[(agent1, agent2)],
    load_threads_callback=lambda: load_threads(chat_id),
    save_threads_callback=lambda thread_dict: save_threads(thread_dict, chat_id),
)
```

<Warning>
If you switch model providers for an existing saved chat, old tool/event items may no longer replay correctly. Start a new chat, or keep only `{role, content}` messages.
</Warning>

</Step>

<Step title="Step 2: Deploy with the Starter Template" icon="rocket-launch">

Use the [Starter Template](/welcome/getting-started/starter-template) as your production base. It already includes FastAPI wiring and deployment defaults.

- Create a repo from the template
- Set `OPENAI_API_KEY` and `APP_TOKEN`
- Follow the template README to deploy

If you are wiring your own server, see [FastAPI Integration](/additional-features/fastapi-integration).

</Step>
</Steps>

<Accordion title="When you need separate tool services" defaultOpen={false}>
If you want to scale tools independently or share them across projects, host tools as APIs and connect them with [OpenAPI schemas](/core-framework/tools/openapi-schemas). This adds setup and requires explicit auth, and shared in-memory state will not persist between tool calls.
</Accordion>
